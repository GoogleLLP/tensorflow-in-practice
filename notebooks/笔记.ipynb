{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>文本深度学习</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T11:54:23.436714Z",
     "start_time": "2021-01-09T11:54:23.431715Z"
    }
   },
   "source": [
    "<h2>句子序列相互转换</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T11:55:15.717480Z",
     "start_time": "2021-01-09T11:55:15.712480Z"
    }
   },
   "source": [
    "<h3>导入包</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T11:54:45.903516Z",
     "start_time": "2021-01-09T11:54:45.900510Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>训练token</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T11:56:12.399260Z",
     "start_time": "2021-01-09T11:56:12.395263Z"
    }
   },
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"I love my dog\",\n",
    "    \"I love mu cat\",\n",
    "    \"I love Gu YUting previously\"\n",
    "]\n",
    "token = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "token.fit_on_texts(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T11:50:27.750230Z",
     "start_time": "2021-01-09T11:50:27.745229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'dog': 5, 'mu': 6, 'cat': 7, 'gu': 8, 'yuting': 9, 'previously': 10}\n"
     ]
    }
   ],
   "source": [
    "print(token.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>使用token将句子转换成序列</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T12:06:32.480650Z",
     "start_time": "2021-01-09T12:06:32.476650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[[2, 3, 4, 5], [2, 3, 6, 7], [2, 3, 8, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "seq = token.texts_to_sequences(words)\n",
    "print(type(seq))         # 填充之前是list类型\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>使用token将序列转换成句子</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T11:58:06.411208Z",
     "start_time": "2021-01-09T11:58:06.407211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i love my dog', 'i love mu cat', 'i love gu yuting previously']\n"
     ]
    }
   ],
   "source": [
    "print(token.sequences_to_texts(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>对序列进行填充</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T12:06:02.569905Z",
     "start_time": "2021-01-09T12:06:02.562881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[ 0  2  3  4  5]\n",
      " [ 0  2  3  6  7]\n",
      " [ 2  3  8  9 10]]\n"
     ]
    }
   ],
   "source": [
    "seq_pad = pad_sequences(\n",
    "    sequences=seq,\n",
    "    maxlen=5,           # 设置最长句子长度\n",
    "    padding=\"pre\",       # 设置在前面补零(pre)，还是在后面补零(post)\n",
    "    truncating=\"post\"    # 如果超出长度，是去掉前面(pre)，还是去掉后面(post)\n",
    ")\n",
    "print(type(seq_pad))   # 填充之后变成numpy类型\n",
    "print(seq_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>使用数据</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>加载数据</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T12:49:50.750635Z",
     "start_time": "2021-01-10T12:49:47.354645Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tdfs\n",
    "imdb, info = tdfs.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>查看训练集测试集</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T12:49:55.692154Z",
     "start_time": "2021-01-10T12:49:55.687154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
     ]
    }
   ],
   "source": [
    "train_data = imdb[\"train\"]\n",
    "print(type(train_data))\n",
    "test_data = imdb[\"test\"]\n",
    "print(type(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>分割标签</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T12:50:33.283262Z",
     "start_time": "2021-01-10T12:50:28.815256Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sequences = []\n",
    "train_labels = []\n",
    "test_sequences = []\n",
    "test_labels = []\n",
    "for s, l in train_data:\n",
    "    train_sequences.append(s.numpy().decode(\"utf8\"))\n",
    "    train_labels.append(l.numpy())\n",
    "for s, l in test_data:\n",
    "    test_sequences.append(str(s.numpy().decode(\"utf8\")))\n",
    "    test_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T12:50:36.196506Z",
     "start_time": "2021-01-10T12:50:36.192509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\n",
      "<class 'str'>\n",
      "1\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(test_sequences[0])\n",
    "print(type(test_sequences[0]))\n",
    "print(test_labels[0])\n",
    "print(type(test_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>查看数据的类型转换过程</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>s为EagerTensor类型</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T12:51:45.657015Z",
     "start_time": "2021-01-10T12:51:45.654017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b\"They just don't make cartoons like they used to. This one had wit, great characters, and the greatest ensemble of voice over artists ever assembled for a daytime cartoon show. This still remains as one of the highest rated daytime cartoon shows, and one of the most honored, winning several Emmy Awards.\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(type(s))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>转换为dtype指定类型</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T12:52:26.305915Z",
     "start_time": "2021-01-10T12:52:26.301899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "b\"They just don't make cartoons like they used to. This one had wit, great characters, and the greatest ensemble of voice over artists ever assembled for a daytime cartoon show. This still remains as one of the highest rated daytime cartoon shows, and one of the most honored, winning several Emmy Awards.\"\n"
     ]
    }
   ],
   "source": [
    "print(type(s.numpy()))\n",
    "print(s.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>编码成字符串</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T12:53:30.554227Z",
     "start_time": "2021-01-10T12:53:30.550230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "They just don't make cartoons like they used to. This one had wit, great characters, and the greatest ensemble of voice over artists ever assembled for a daytime cartoon show. This still remains as one of the highest rated daytime cartoon shows, and one of the most honored, winning several Emmy Awards.\n"
     ]
    }
   ],
   "source": [
    "print(type(s.numpy().decode(\"utf8\")))\n",
    "print(s.numpy().decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>查看标签的类型转换过程</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>l为EagerTensor类型</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T12:57:35.552007Z",
     "start_time": "2021-01-10T12:57:35.548857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(type(l))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>转换为dtype指定类型</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(l.numpy()))\n",
    "print(l.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
